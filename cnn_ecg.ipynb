{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af3c0cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfdb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt\n",
    "import csv\n",
    "import os\n",
    "from wfdb import processing\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d23b6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise(data):\n",
    "    # 小波变换\n",
    "    coeffs = pywt.wavedec(data=data, wavelet='db5', level=9)\n",
    "    cA9, cD9, cD8, cD7, cD6, cD5, cD4, cD3, cD2, cD1 = coeffs\n",
    "\n",
    "    # 阈值去噪\n",
    "    threshold = (np.median(np.abs(cD1)) / 0.6745) * (np.sqrt(2 * np.log(len(cD1))))\n",
    "    cD1.fill(0)\n",
    "    cD2.fill(0)\n",
    "    for i in range(1, len(coeffs) - 2):\n",
    "        coeffs[i] = pywt.threshold(coeffs[i], threshold)\n",
    "\n",
    "    # 小波反变换,获取去噪后的信号\n",
    "    rdata = pywt.waverec(coeffs=coeffs, wavelet='db5')\n",
    "    return rdata\n",
    "\n",
    "\n",
    "def find_peak(file_addr, record):\n",
    "#     record = wfdb.rdrecord(file_addr, sampfrom=0)\n",
    "\n",
    "    # Use the GQRS algorithm to detect QRS locations in the first channel\n",
    "    qrs_inds = processing.qrs.gqrs_detect(sig=record.p_signal[:, 0], fs=record.fs)\n",
    "\n",
    "    # Correct the peaks shifting them to local maxima\n",
    "    min_bpm = 20\n",
    "    max_bpm = 230\n",
    "    # min_gap = record.fs * 60 / min_bpm\n",
    "    # Use the maximum possible bpm as the search radius\n",
    "    search_radius = int(record.fs * 60 / max_bpm)\n",
    "    corrected_peak_inds = processing.peaks.correct_peaks(record.p_signal[:, 0],\n",
    "                                                         peak_inds=qrs_inds,\n",
    "                                                         search_radius=search_radius,\n",
    "                                                         smooth_window_size=150)\n",
    "\n",
    "    return sorted(corrected_peak_inds)\n",
    "\n",
    "\n",
    "def get_data(file_addr, X_data, Y_data):\n",
    "    print(\"reading \" + file_addr + \" data....\")\n",
    "    record = wfdb.rdrecord(\"training2017/\"+file_addr, sampfrom=0)\n",
    "    peak_index = find_peak(file_addr, record)\n",
    "    data = record.p_signal.flatten()\n",
    "    rdata = denoise(data=data)\n",
    "#     start = 10\n",
    "#     end = 5\n",
    "#     i = start\n",
    "#     j = len(peak_index) - end\n",
    "    \n",
    "    x_train=rdata[peak_index[1]-99:peak_index[1]+201]\n",
    "    X_data.append(x_train)\n",
    "    \n",
    "    \n",
    "def loadData():\n",
    "    #read csv file     \n",
    "    with open(\"training2017/test.csv\") as f:\n",
    "        file_addr_list = []\n",
    "        ecg_type_list = []\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            file_addr_list.append(row[0])\n",
    "            ecg_type_list.append(row[1])\n",
    "        \n",
    "    dataSet = []\n",
    "    lableSet = ecg_type_list\n",
    "    \n",
    "    for n in file_addr_list:\n",
    "        get_data(n, dataSet, lableSet)\n",
    "         \n",
    "    # 转numpy数组,打乱顺序\n",
    "    dataSet = np.array(dataSet)\n",
    "    lableSet = np.array(lableSet)\n",
    "    dataSet = np.array(dataSet).reshape(-1, 270)\n",
    "    lableSet = np.array(lableSet).reshape(-1, 1)\n",
    "    train_ds = np.hstack((dataSet, lableSet))\n",
    "    np.random.shuffle(train_ds)\n",
    "    \n",
    "    X = train_ds[:, :300].reshape(-1, 300, 1)\n",
    "    Y = train_ds[:, 300]\n",
    "    RATIO = 0.3\n",
    "    shuffle_index = np.random.permutation(len(X))\n",
    "    test_length = int(RATIO * len(shuffle_index)) # RATIO = 0.3\n",
    "    test_index = shuffle_index[:test_length]\n",
    "    train_index = shuffle_index[test_length:]\n",
    "    X_test, Y_test = X[test_index], Y[test_index]\n",
    "    X_train, Y_train = X[train_index], Y[train_index]\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "#     return dataSet, lableSet\n",
    "\n",
    "\n",
    "def loadtestData():\n",
    "    #read csv file     \n",
    "    with open(\"validation/test.csv\") as f:\n",
    "        file_addr_list = []\n",
    "        ecg_type_list = []\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            file_addr_list.append(row[0])\n",
    "            ecg_type_list.append(row[1])\n",
    "        \n",
    "    dataSet = []\n",
    "    lableSet = ecg_type_list\n",
    "    \n",
    "    for n in file_addr_list:\n",
    "        get_data(n, dataSet, lableSet)\n",
    "    dataSet = np.array(dataSet)\n",
    "    lableSet = np.array(lableSet)\n",
    "    return dataSet, lableSet\n",
    "\n",
    "\n",
    "def buildModel():\n",
    "    newModel = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=(300, 1)),\n",
    "        # 第一个卷积层, 4 个 21x1 卷积核\n",
    "        tf.keras.layers.Conv1D(filters=4, kernel_size=21, strides=1, padding='SAME', activation='relu'),\n",
    "        # 第一个池化层, 最大池化,4 个 3x1 卷积核, 步长为 2\n",
    "        tf.keras.layers.MaxPool1D(pool_size=3, strides=2, padding='SAME'),\n",
    "        # 第二个卷积层, 16 个 23x1 卷积核\n",
    "        tf.keras.layers.Conv1D(filters=16, kernel_size=23, strides=1, padding='SAME', activation='relu'),\n",
    "        # 第二个池化层, 最大池化,4 个 3x1 卷积核, 步长为 2\n",
    "        tf.keras.layers.MaxPool1D(pool_size=3, strides=2, padding='SAME'),\n",
    "        # 第三个卷积层, 32 个 25x1 卷积核\n",
    "        tf.keras.layers.Conv1D(filters=32, kernel_size=25, strides=1, padding='SAME', activation='relu'),\n",
    "        # 第三个池化层, 平均池化,4 个 3x1 卷积核, 步长为 2\n",
    "        tf.keras.layers.AvgPool1D(pool_size=3, strides=2, padding='SAME'),\n",
    "        # 第四个卷积层, 64 个 27x1 卷积核\n",
    "        tf.keras.layers.Conv1D(filters=64, kernel_size=27, strides=1, padding='SAME', activation='relu'),\n",
    "        # 打平层,方便全连接层处理\n",
    "        tf.keras.layers.Flatten(),\n",
    "        # 全连接层,128 个节点\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        # Dropout层,dropout = 0.2\n",
    "        tf.keras.layers.Dropout(rate=0.2),\n",
    "        # 全连接层,5 个节点\n",
    "        tf.keras.layers.Dense(5, activation='softmax')\n",
    "    ])\n",
    "    return newModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9d0bbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading A00001 data....\n",
      "reading A00002 data....\n",
      "reading A00003 data....\n",
      "reading A00004 data....\n",
      "reading A00005 data....\n",
      "reading A00006 data....\n",
      "reading A00007 data....\n",
      "reading A00008 data....\n",
      "reading A00009 data....\n",
      "reading A00010 data....\n",
      "reading A00011 data....\n",
      "reading A00012 data....\n",
      "reading A00013 data....\n",
      "reading A00014 data....\n",
      "reading A00015 data....\n",
      "reading A00016 data....\n",
      "reading A00017 data....\n",
      "reading A00018 data....\n",
      "reading A00019 data....\n",
      "reading A00020 data....\n",
      "reading A00021 data....\n",
      "reading A00022 data....\n",
      "reading A00023 data....\n",
      "reading A00024 data....\n",
      "reading A00025 data....\n",
      "reading A00026 data....\n",
      "reading A00027 data....\n",
      "reading A00028 data....\n",
      "reading A00029 data....\n",
      "reading A00030 data....\n",
      "reading A00031 data....\n",
      "reading A00032 data....\n",
      "reading A00033 data....\n",
      "reading A00034 data....\n",
      "reading A00035 data....\n",
      "reading A00036 data....\n",
      "reading A00037 data....\n",
      "reading A00038 data....\n",
      "reading A00039 data....\n",
      "reading A00040 data....\n",
      "reading A00041 data....\n",
      "reading A00042 data....\n",
      "reading A00043 data....\n",
      "reading A00044 data....\n",
      "reading A00045 data....\n",
      "reading A00046 data....\n",
      "reading A00047 data....\n",
      "reading A00048 data....\n",
      "reading A00049 data....\n",
      "reading A00050 data....\n",
      "reading A00051 data....\n",
      "reading A00052 data....\n",
      "reading A00053 data....\n",
      "reading A00054 data....\n",
      "reading A00055 data....\n",
      "reading A00056 data....\n",
      "reading A00057 data....\n",
      "reading A00058 data....\n",
      "reading A00059 data....\n",
      "reading A00060 data....\n",
      "reading A00061 data....\n",
      "reading A00062 data....\n",
      "reading A00063 data....\n",
      "reading A00064 data....\n",
      "reading A00065 data....\n",
      "reading A00066 data....\n",
      "reading A00067 data....\n",
      "reading A00068 data....\n",
      "reading A00069 data....\n",
      "reading A00070 data....\n",
      "reading A00071 data....\n",
      "reading A00072 data....\n",
      "reading A00073 data....\n",
      "reading A00074 data....\n",
      "reading A00075 data....\n",
      "reading A00076 data....\n",
      "reading A00077 data....\n",
      "reading A00078 data....\n",
      "reading A00079 data....\n",
      "reading A00080 data....\n",
      "reading A00081 data....\n",
      "reading A00082 data....\n",
      "reading A00083 data....\n",
      "reading A00084 data....\n",
      "reading A00085 data....\n",
      "reading A00086 data....\n",
      "reading A00087 data....\n",
      "reading A00088 data....\n",
      "reading A00089 data....\n",
      "reading A00090 data....\n",
      "reading A00091 data....\n",
      "reading A00092 data....\n",
      "reading A00093 data....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda\\envs\\ml_env\\lib\\site-packages\\pywt\\_multilevel.py:45: UserWarning: Level value of 9 is too high: all coefficients will experience boundary effects.\n",
      "  \"boundary effects.\").format(level))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading A00094 data....\n",
      "reading A00095 data....\n",
      "reading A00096 data....\n",
      "reading A00097 data....\n",
      "reading A00098 data....\n",
      "reading A00099 data....\n",
      "reading A00100 data....\n",
      "reading A00101 data....\n",
      "reading A00102 data....\n",
      "reading A00103 data....\n",
      "reading A00104 data....\n",
      "reading A00105 data....\n",
      "reading A00106 data....\n",
      "reading A00107 data....\n",
      "reading A00108 data....\n",
      "reading A00109 data....\n",
      "reading A00110 data....\n",
      "reading A00111 data....\n",
      "reading A00112 data....\n",
      "reading A00113 data....\n",
      "reading A00114 data....\n",
      "reading A00115 data....\n",
      "reading A00116 data....\n",
      "reading A00117 data....\n",
      "reading A00118 data....\n",
      "reading A00119 data....\n",
      "reading A00120 data....\n",
      "reading A00121 data....\n",
      "reading A00122 data....\n",
      "reading A00123 data....\n",
      "reading A00124 data....\n",
      "reading A00125 data....\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 37500 into shape (270)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-6ea833a41190>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# X_train,Y_train为所有的数据集和标签集\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# X_test,Y_test为拆分的测试集和标签集\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloadData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m# # 构建CNN模型\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# model = buildModel()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-e3e3946d7bad>\u001b[0m in \u001b[0;36mloadData\u001b[1;34m()\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[0mdataSet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataSet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[0mlableSet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlableSet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m     \u001b[0mdataSet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataSet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m270\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[0mlableSet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlableSet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[0mtrain_ds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataSet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlableSet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 37500 into shape (270)"
     ]
    }
   ],
   "source": [
    "# X_train,Y_train为所有的数据集和标签集\n",
    "# X_test,Y_test为拆分的测试集和标签集\n",
    "X_train, Y_train, X_test, Y_test = loadData()\n",
    "# # 构建CNN模型\n",
    "# model = buildModel()\n",
    "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# model.summary()\n",
    "# # 定义TensorBoard对象\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "# # 训练与验证\n",
    "# model.fit(X_train, Y_train, epochs=30, batch_size=128, validation_split=RATIO, callbacks=[tensorboard_callback])\n",
    "# model.save(filepath=model_path)\n",
    "\n",
    "# # 预测\n",
    "# Y_pred = model.predict_classes(X_test)\n",
    "# Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9324902c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fb4b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_images, train_labels = loadData()\n",
    "# test_images, test_labels = loadtestData()\n",
    "# model = models.Sequential()\n",
    "# model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(layers.MaxPooling2D((2, 2)))\n",
    "# model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "# model.add(layers.Flatten())\n",
    "# model.add(layers.Dense(64, activation='relu'))\n",
    "# model.add(layers.Dense(10))\n",
    "\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# history = model.fit(train_images, train_labels, epochs=10, \n",
    "#                     validation_data=(test_images, test_labels))\n",
    "\n",
    "# print(test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
